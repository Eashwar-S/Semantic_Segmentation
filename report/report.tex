\documentclass{IEEEtran}
\usepackage[english]{babel}
\usepackage{caption}
\usepackage[export]{adjustbox}
%\usepackage{comment}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{authblk}
\graphicspath{{images/}}
\usepackage{geometry}
\usepackage{array}
\usepackage{color,soul}
\usepackage{tabularx}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=15mm,
 top=15mm,
 }
\title{Semantic Segmentation using Convolutional Neural Network}
\author{
\begin{tabular}[t]{c@{\extracolsep{8em}}c} 
Eashwar Sathyamurthy  \hspace{2in}Dr. Mohammed Charifa \\
Robotics Engineer \hspace{2in} Supervising Faculty \\ 
A. James Clark School of Engineering \hspace{1in} Maryland Applied Graduate Engineering \\
University of Maryland \hspace{2in} University of Maryland \\
College Park, USA \hspace{2in} College Park, USA \\
eashwar@umd.edu \hspace{2in} scharifa@umd.edu
\end{tabular}
}

\begin{document}
\maketitle
\begin{abstract}
Human beings are easily able to identify and differentiate objects based on their cognitive skills. Cognitive skills like category formation, pattern recognition, working memory etc help humans to segment and process visual information obtained through eyes. But, a machine process visual data differently in the form of matrix containing whole numbers known as pixels. Each pixel indicates the brightness levels at that particular location. As the machine does not posses cognitive capability to segment images, a convolution neural net is designed and built through which the machine learns to segment images. The segmentation performed is called semantic segmentation as it groups every pixel of the image into classes based on labels. The report describes and implements convolutional neural networks like U-net, Segnet, IC-net and Resnet34, compare the results obtained and recommends the best architecture for semantic segmentation.
\end{abstract}

\section{\textbf{Introduction}}
Over the years convolution neural net (CNN) has increased its horizon and made significant impact on pattern recognition and machine learning fields. CNN is one type of neural network which has one input layer, one output layer and two or more hidden layers. The hidden layers is a combination of convolution layers, pooling layers, activation functions, deconvolution layers, unpooling layers and a fully connected layer. The main distinction between a CNN and a regular neural net is that in CNN neurons handles input and output in three dimensions. An example of CNN is shown in the figure below[1]:
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{CNN}
    \caption{CNN Example}
    \label{fig:CNN Example}
\end{figure}

\section{Definitions}
In this section, different layers of a CNN are explained in detail.
\subsection{\textbf{Input Layer}}
The input to the input layer of the CNN is generally a color image represented in three dimensional arrays containing pixels in range 0-255. The three dimensions are length, width and number of channels (R,G,B) of the input image. Generally, the input image is resized to the desired dimensions accepted by the CNN. 

\subsection{\textbf{Convolutional Layer}}
In the convolutional layer, convolution operation happens between the kernel and the input image. Kernel is a 2D array whose elements are normally distributed. An example of 3x3 Gaussian Kernel is given below[2]:
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=3cm]{Guassian kernel}
    \caption{3x3 Gaussian kernel }
    \label{fig:3x3 Guassian kernel}
\end{figure}

Mathematically, 2D convolution is represented as follows:
\begin{equation*}
\boldsymbol{y[i,j] = \sum_{m=-\infty}^{\infty}\sum_{n=-\infty}^{\infty} h[m,n] . x[i-m, j-n]}
\end{equation*}

In convolution layer, the convolution operation is performed on all the pixels by sliding the kernel over the input image. The following image shows the convolution operation between the image and the kernel[3]: 
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=10cm]{conv2d}
    \caption{Convolving kernel over the image}
    \label{fig:3x3 Guassian kernel}
\end{figure}

Typically, in most neural networks the dimensions of the image after performing convolution are maintained same with the help of padding. But, it is important to understand how convolution operation affects the dimensions of the image. The relation is given as follows:
\begin{equation*}
	\boldsymbol{Output Width = \frac{I_W - K_W + 2*P}{S} + 1}
\end{equation*}
\begin{center}
where  $I_W$ is the image width \\
$K_W$ is the kernel width	\\	
P is number of padding bits	\\
S = stride = number of pixels shift over the input image
\end{center}
\begin{equation*}
	\boldsymbol{Output Height = \frac{I_H - K_H + 2*P}{S} + 1}
\end{equation*}
\begin{center}
where  $I_H$ is the image height \\
$K_H$ is the kernel height	\\	
P is number of padding bits
\end{center}

\subsection{\textbf{Pooling Layers:}}
Pooling layers in a CNN are typically connected after convolutional layers. It reduces the size of the image by retaining only the important features in the image and neglecting the rest. Pooling layers have the following advantages:
\begin{enumerate}
\item It reduces the number of parameters to train in a CNN.

\item It prevents the neural net from the over-fitting problem. Over fitting is a situation where the neural net obtains high accuracy over training data and less accuracy over the testing data. This usually occurs because of large number of trainable parameters which lets the neural net detect all the patterns in an training image present but fails to learn from the patterns to generalize it on the testing image. This can be observed when the neural net obtains more than 95\% accuracy on the training data but less than 50\% accuracy on the testing data.
\newline

\textbf{Commonly used Pooling Layers:} Average pooling and max pooling are the two most commonly used pooling layers in neural network.
\begin{enumerate}
\item \textbf{Average Pooling:} In average pooling, the average value is selected from each patch of the feature map to obtained a reduced map size. The following image gives an example of average pooling[4]:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{avgpool}
    \caption{Average Pooling Example}
    \label{fig:Average Pooling Example}
\end{figure}

In the above example, the feature map is of size 4x4 and the patch is of size 2x2 and stride = 2 and there are no padding bits. So, if we apply the formula for resulting dimension of the feature map, we get the following:
\begin{equation*}
\begin{aligned}
Output Width &= \frac{I_W - K_W + 2*P}{S} + 1 \\
			&= \frac{4 - 2 + 2*0}{2} + 1 \\
Output Width&= 2 
\end{aligned} 
\end{equation*}
\begin{equation*}
\begin{aligned}
Output Height &= \frac{I_H - K_H + 2*P}{S} + 1 \\
			&= \frac{4 - 2 + 2*0}{2} + 1 \\
Output Height&= 2 
\end{aligned} 
\end{equation*}
So, the output feature map will be of size 2x2. Let consider the first (violet) patch and calculate the average pooling output:
\begin{equation*}
Output value = \frac{4 + 9 + 5 + 6}{4} = 6.0
\end{equation*}
This process is repeated over the entire feature map to obtain the reduced 2x2 feature map output.

\item \textbf{Max Pooling:} In max pooling, the maximum value is selected from each patch of the feature map to obtained a reduced map size. Max pooling is the most commonly used pooling layers.  The following image gives an example of max pooling[4]:
 
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{maxpool}
    \caption{Max Pooling Example}
    \label{fig:Max Pooling Example}
\end{figure}

As this is the same example used in average pooling, the output feature map size is 2x2. The following is the max pooling output for the first (violet) patch of the feature map:
\begin{equation*}
Output value = max(4,9,5,6) = 9.
\end{equation*}
\end{enumerate}
\end{enumerate}

\subsection{\textbf{Unpooling Layers:}}
In CNN, inverse operation of pooling is not possible. But unpooling layer obtains the approximate inverse of pooling operation by storing the index of the maximum value from each patch of the feature map in a set of switch variables while pooling operation is being performed. These switch variables  are again used in unpooling layers to reconstruct the layer before the pooling operation was performed. The below picture shows unpooling:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{unpool}
    \caption{}
    \label{fig:}
\end{figure}
  

\subsection{\textbf{Activation Functions:}}
As the name suggests, an activation function decides whether to activate a neuron or not based on calculations of weighted sum and bias. In the case of a CNN, weighted sum is the output image obtained by convolving the kernel over the input image. Bias is a fixed value added pixel wise to the obtained output image. Activation function is applied to this output image which decides whether to select a particular pixel value and pass on to the next layer or not. 
\newline
\textbf{Need for Activation Functions:} In neural networks, the weights (kernel in the case of CNN) and bias values constantly gets updated based on the error at the output. The values are updated using back-propagation algorithm. Activation functions makes the back propagation possible as the gradients are supplied along with errors to update the weight and bias values[5].
\newline
\textbf{Different Activation Functions:} There are different activation functions used in neural networks based on its applications. They are:
\begin{enumerate}
\item \textbf{Step Function:} Step function is a threshold based activation function which outputs 1 for the values greater than a specified threshold or outputs 0 otherwise. Hence, 1 means the neuron gets activated and 0 means neuron gets deactivated. It is shown in the below figure:[7]
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{step}
    \caption{Step Function}
    \label{fig: Step Function}
\end{figure}
The following conditional equations defines step function:
\begin{equation*}
\begin{aligned}
\text{Activation Function  } A &= 1 \text{    if    } y \geq threshold \\
						&= 0 \text{    otherwise}
\end{aligned}
\end{equation*}
Step function can be used as an activation function if the neural net is being used for binary classification where the output is just "YES" or "NO". Typical example of such classification is determining the picture is of a dog or a cat. 

But, step functions cannot be used as activation functions for multiple(more than 2) classifications as the neural net might give multiple 1's and 0's as output.

\item \textbf{Linear Function:} Linear function means a straight line functions which is proportional to the input. The below figure shows the example:[7]
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{lin}
    \caption{Linear Function}
    \label{fig: Linear Function}
\end{figure}
The following conditional equations defines linear function:
\begin{equation*}
\text{Activation Function  } A = c*X
\end{equation*}
Linear function eliminates the multiple classification problem faced by employing step function as the activation function. As the activation function varies with input values, it can take many values not only 0 and 1. Hence, it can classify multiple classes.

The main disadvantage of linear function is the derivative becomes constant. This means that the gradient no longer varies with input. Hence,  if there was an error in detecting the output, since the gradient is constant there will be constant change in the weights and bias values irrespective of inputs. This means that the neural net will fail to learn.

Another disadvantage occurs due to linearity property. By, employing linear functions as activation functions to all hidden layers, the output will be a linear combination of hidden layers. Hence, multiple hidden layers can be combined into 1 hidden layer. Hence,  by employing linear function, 1-hidden layer neural nets are possible.
\newline
\item \textbf{Sigmoid Function:} The below picture shows the sigmoid function.[7]
\newpage

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{sig}
    \caption{Linear Function}
    \label{fig: Linear Function}
\end{figure}
The following conditional equations defines sigmoid function:
\begin{equation*}
\text{Activation Function  } A = \frac{1}{1 + e^{-x}}
\end{equation*}

The sigmoid function is essentially a smoothened version of step function. This makes it far more useful than step function. Firstly, sigmoid function is non-linear which eliminates the linearity problem. Secondly, it can take many values between 0 and 1 which makes it usable in multi class classification problems.

The only disadvantage of sigmoid function is that gradient of the points farther from origin approaches to zero which gives rise to vanishing gradients problem. This makes the neural net to learn very slowly and in worst case scenarios not learn at all.

\item \textbf{Tanh Function:} The below picture shows the tanh function.[7]

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{tanh}
    \caption{Tanh Function}
    \label{fig: Tanh Function}
\end{figure}
The following conditional equations defines tanh function:
\begin{equation*}
\text{Activation Function  } A = \frac{2}{1 + e^{-2*x}} - 1
\end{equation*}

Tanh function is a scaled version of sigmoid function. The gradient of the points farther from origin approaches slowly to zero than sigmoid function. But, tanh function also faces the problem of vanishing gradient.

\item \textbf{Relu:} Relu stands for Rectified linear unit. It is one of the most commonly used activation function. The figure below shows the function:[7]
\newpage

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{relu}
    \caption{Relu Function}
    \label{fig: Relu Function}
\end{figure}

Relu is non-linear in nature as it remains zero for all the negative values which allows neural net to have multiple layers. In addition to this, most of the neurons in the neural net never fire up due to the characteristics of Relu function. This makes the neural net lighter. This makes Relu function less computationally expensive than Sigmoid and Tanh functions.

The disadvantage of Relu function is that for negative values the gradient remains remains zero. This means that some neurons in the neural net will not respond to the change in the output. This is called dying Relu problem.
\end{enumerate} 

\subsection{\textbf{Fully Connected Layer:}}
In neural network, a fully connected later is that layer which connects the inputs of one layer to every activation unit of the next layer. Typically, a fully connected layer is present before the output layer. In a neural net each unit in a convolutional layer contains a feature of the input image essentially an edge. Hence, each convolutional layer is a collection of features of the input image. A fully connected layer holds the composite and aggregate features of all the convolution layers. Hence, a fully connected layer is a feature vector of the input image. The following figure shows an example of a fully connected layer:[8]

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=5cm]{fcn}
    \caption{Fully Connected Layer}
    \label{fig: Fully Connected Layer}
\end{figure}

\subsection{\textbf{Deconvolution Layer:}}
Deconvolution layers in CNN is used to represent the output data in the same dimensions as the input image but increasing the size of the output data. Deconvolution is a transpose operation of convolution. 

In deconvolution, first the data is padded and separately according to the desired output size. The below figure shows the representation of data in padded and separated form to convert data of size 2x2 to 4x4:[11]

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{deconv1}
    \caption{}
    \label{fig:}
\end{figure}
 
Then the kernel is made to slide over the padded representation to get 4x4 output data size.[12]

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=10cm]{deconv2}
    \caption{}
    \label{fig:}
\end{figure}


\subsection{\textbf{Output Layer:}}
The output layer is the final layer of CNN. The output layer takes input from the fully connected layer and assigns probability to it. The assignment of probability is done through softmax activation function. Hence, the output layer outputs the probability of each class in the input image. The below image shows the softmax activation function present in output layer.[10]

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8cm]{softmax}
    \caption{Softmax Activation Function Example}
    \label{fig:Softmax Activation Function Example}
\end{figure}
\newpage
\section{\textbf{U-net Architecture}}
U-net is popularly used CNN for segmentation. The name U-net comes from the fact that the layers in the CNN are arranged in U shape. The below figure shows the U-net architecture[17]:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=10cm]{u-net}
    \caption{U-net Architecture}
    \label{fig:U-net Architecture}
\end{figure}

The U-net architecture consists of two path. They are:\\

1. \textbf{Contraction Path}: The left half of the U-net where the layers are represented from top to bottom is called contraction path. The contraction path consists of an input layer, sequence of convolution layers, Relu activation layers and max pooling layers. These series of layers reduce the size of the input image simultaneously increasing the number of channels.


2. \textbf{Expansion Path}: The right half of the U-net where the layers are represented from bottom to top is called expansion path. The expansion path consists of sequence of deconvolution or up-convolution layers and concatenation with high resolution features from the contraction path and unpooling layers. The expansion path gives output data in the same size as the input image data. The expansion path increase the size of the contraction path output data simultaneously decreasing the number of channels.

\section{\textbf{Segnet Architecture}}
Segnet is designed to perform pixel wise segmentation more efficiently. It consists of an encoder network followed by a decoder network and finally a pixel classification layer. The encoder layer consists of 13 convolution layers similar to VGG16 for object classification. Since each encoder layer has a corresponding decoder layer, there are 13 convolution layers present in decoder layer.

Each encoder layer performs convolution operation to detect the features of the input image. Then, these are batch normalized to reduce the amount by which hidden unit values shift. Finally, a rectified linear unit (ReLU) activation is applied pixel wise followed by max pooling operation. Each decoder layer has a upsampling layer to perform unpooling operation and also to increase the image size by a factor 2 followed by convolution layers. The last decoder layer has a softmax activation function which gives output images whose length is the number of segmented labels present. The figure below shows the segnet architecture[15]:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{segnet}
    \caption{Segnet Architecture}
    \label{fig:Binary class segmented output}
\end{figure}

\section{\textbf{IC-net Architecture}}
The full form of IC-net is image cascade network. IC-net architecture is designed to make sematic segmentation faster while not compromising too much on the accuracy. IC-net is a highly efficient semantic segmentation system with descent quality. IC-net architecture is shown below[18]:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=9cm]{ic-net.JPG}
    \caption{IC-net Architecture}
    \label{fig:Binary class segmented output}
\end{figure}

IC-net architecture takes cascade image inputs i.e the resolution of the input image is varied into three categories of low, medium and high resolution images and are fed as inputs to the architecture in a cascaded form. As the semantic segmentation using of resolution images is computationally expensive and slow, the low resolution image (1/4 of original image) is fed into PSPnet architecture for semantic segmentation. The PSPnet architecture outputs segmented image with resolution 1/32 of the original image. As we decreased the resolution of the image, the quality of semantic segmentation is reduced. To preserve the quality, the medium (1/2 of the original image) and high (original image) resolution image are used to refine the prediction process. The medium and high resolution images are passed through light weighted CNNs which are less computationally expensive and takes less to extract features. Thus, the outputs obtained from different resolution branches are fused using cascaded feature fusion unit (CFF) and trained using cascade label guidance.
\newline
\\
\textbf{Cascaded Feature Fusion Unit:}
Cascaded Feature Fusion Unit combines the features extracted from the outputs obtained from different CNN architectures of different resolution and outputs combined feature vector with the desired resolution. The unit takes 2 input feature maps (F1 and F2) and 1 ground truth label (LABEL) as input. First, the unit converts the feature map F1 into same resolution by performing upsampling operation.Then refinement of features is done through dilation convolution operation. The feature map F2 undergoes projection convolution wuth 1x1 kernel to ouput same number of channels as feature map F1. Then, the two feature maps undergoes element wise sum in SUM layer followed by 'RELU' layer to obtained a fused feature map[18]. 
\newline
\\
\textbf{Cascaded Label Guidance Unit:}
Cascaded Label Guidance strategy is used to enhance the learning of the IC-net architecture. It uses different resolutions (low, medium and high) of ground truth label for different resolution branches(low, medium and high). The weighted softmax cross entropy loss is calculated for each resolution branch with relative loss weight. This ultimately minimized the loss function of the IC-net architecture[18].

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{cee.JPG}
    \caption{Cascaded Feature Fusion}
    \label{fig:Binary class segmented output}
\end{figure}

\section{\textbf{Resnet34 Architecture}}
The full form of Resnet is Residual network and the number 34 signifies the number of layers in the Resnet architecture. The Resnet architecture mainly address the degradation problem which arises with architecture's increasing depth. When the number of layers in the CNN increases the accuracy becomes saturated and begin to decrease drastically. Resnet architecture presents a solution to this problem by introducing residual or shortcut connections. The residual or shortcut connections means to add previous layer output with some train error to output obtained after passing it through the convolutional layer. This ensures that the resulting train error will not increase with increasing depth of the architecture. If the input and output layer have same dimensions, then the shortcut connections are applied directly or the output is padded with zeroes to match input dimension or undergoes 1x1 convolution to match the input layer dimensions. The Resnet34 architecture is shown in the figure below[19]: 

\newpage
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=3.8cm]{resnet.JPG}
    \caption{Resnet Architecture}
    \label{fig:Binary class segmented output}
\end{figure}

\section{\textbf{Implementation:}}
The details of the implementation of the above mentioned architecture and required dependencies are given below:
\begin{enumerate}
\item Language Used: Python version - 3.8

\item Packages Used: Tensorflow version - 2.2.0, Numpy, cv2

\item Software Used: Jupyter Notebook

\item Github Link: \textcolor{blue}{\underline{\url{https://github.com/Eashwar-S/Semantic_Segmentation}}}

\item Cloud Service Used: AWS Sagemaker

\item Cloud Storage used: AWS S3 bucket storage

\end{enumerate}

\section{\textbf{Cityscrapes Dataset:}}
Cityscrapes dataset consisted of 5000 images and labels. 2975 images were used for training, 500 for validation and remaining 1525 images were used for testing. The are a total of 29 classes present in the dataset.

\section{\textbf{Results}}
Semantic segmentation was performed using U-net, Segnet, IC-net and Resnet34 architectures and the below table represents the results obtained with architectures arranged in ascending order based on their performance and accuracy.

\begin{tabular}{ |p{0.6cm}|p{1.8cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.8cm}|p{0.8cm}|}
 \hline
 \multicolumn{7}{|c|}{\textbf{Architecture Results}} \\
 \hline
 \textbf{Sno} & \textbf{Architecture} & \textbf{Img size} & \textbf{batch size} & \textbf{L.R} & \textbf{IOU Accu.} & \textbf{Loss} \\
 \hline
 1. & U-net   & 256 x256    & 16  & 0.001 & 53.29\% & 0.3350 \\
 \hline
 2. & IC-net   & 256 x256    & 16 & 0.001 & 48.39\% & 0.4134 \\
 \hline
 3. & Segnet   & 256 x256    & 16  & 0.001 & 48.39\% & 0.5001 \\
 \hline
 4. & Resnet34   & 256 x256    & 16  & 0.001 & 47.37\% & 0.4943 \\
 \hline
\end{tabular}

\subsection{\textbf{Segmented Output}} 
The following are the results of the segmented output obtained by U-net, IC-net, segnet and resnet architectures for the same input image are given below respectively:
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.1cm]{seg.JPG}
    \caption{Segmented Results}
    \label{fig:Binary class segmented output}
\end{figure}
\newpage

\subsection{\textbf{MIOU accuracy and Loss}} 
The following are the graphs of MIOU accuracy and categorical cross entropy loss with respect to number of EPOCHS. The left half of the graph represents the result of training and the right half is for validation.

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{U-net-cityscrapes-B16-gr.JPG}
    \caption{U-net MIOU accuracy and Loss}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{Segnet-cityscrapes-B16-gr.JPG}
    \caption{Segnet MIOU accuracy and Loss}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{ICnet-cityscrapes-B16-gr.JPG}
    \caption{ICnet MIOU accuracy and Loss}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{Resnet-cityscrapes-B16-gr.JPG}
    \caption{Resnet MIOU accuracy and Loss}
    \label{fig:Binary class segmented output}
\end{figure}
\newpage

\subsection{\textbf{Precision vs Recall Graph}}
The following are the graphs of Precision vs Recall for 4 architectures. The left half of the graph represents the result of training and the right half is for validation.

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{U-net-cityscrapes-B16-p-vs-re.JPG}
    \caption{U-net Precision vs Recall Graph}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{Segnet-cityscrapes-B16-p-vs-re.JPG}
    \caption{Segnet Precision vs Recall Graph}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{ICnet-cityscrapes-B16-p-vs-re.JPG}
    \caption{ICnet Precision vs Recall Graph}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{Resnet-cityscrapes-B16-p-vs-re.JPG}
    \caption{Resnet Precision vs Recall Graph}
    \label{fig:Binary class segmented output}
\end{figure}

\newpage

\subsection{\textbf{IOU accuracy of each class}}
\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7.5cm]{U-net-cityscrapes-B16-IOU-C29.JPG}
    \caption{U-net IOU per class}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7.5cm]{Segnet-cityscrapes-B16-IOU-C29.JPG}
    \caption{Segnet IOU per class}
    \label{fig:Binary class segmented output}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7.5cm]{Resnet-cityscrapes-B16-IOU-C29.JPG}
    \caption{Resnet IOU per class}
    \label{fig:Binary class segmented output}
\end{figure}

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=7cm]{ICnet-cityscrapes-B16-IOU-C29.JPG}
    \caption{IC-net IOU per class}
    \label{fig:Binary class segmented output}
\end{figure}

In all the architectures, class road was detected most accurately.\\
1. U-net : 91.998\% \\
2. Segnet : 89.082\% \\
3. IC-net : 68.287\% \\
4. Resnet : 33.421\% \\
\newpage

\section{\textbf{Amazon Web Services}}
Amazon web services was used to train the neural net models as powerful GPUs were required to perform convolution operation on big dataset like cityscrapes. The dataset was uploaded to Amazon cloud storage service called S3 bucket. Then, using one of the service of AWS called Sagemaker jupyter notebook was launched. The dataset stored on the S3 bucket storage device was made public in order to access the dataset from Sagemaker. The below diagram shows how to dataset is handled:

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=8.5cm]{aws.JPG}
    \caption{Accesing the dataset from cloud}
    \label{fig:Binary class segmented output}
\end{figure}

AWS does not give GPU capabilities to its user by default. Therefore, GPU request need to be sent by the user to the AWS service explaining the need for the GPU. The above mentioned process was followed and the following GPU was allocated:
\begin{enumerate}
    \item GPU name : Tesla 1xK80
    \item Instance type: ml.p2.x.large instance
    \item number of vCPUs : 4
    \item RAM : 61 GB
    \item GPU Memory: 12 GB
\end{enumerate}

\section{\textbf{Conclusion}}
An in depth study of convolutional neural networks used for semantic segmentation is obtained. Different CNN architectures like U-net, Segnet, Resnet and IC-net have been studied and implemented and the results have been compared. By comparing the results, it us seen that U-net performs better than other architectures with a MIOU accuracy of 53.29\%. The reason U-net performs better is because in its expansion path, the concatenation of high resolution images from the contraction takes place. This restores the features lost during the upconvolution operation. Hence, it is recommended to use U-net for multiclass semantic segmentation.

\section{\textbf{References:}}
\begin{enumerate}
\item Layers of a Convolutional Neural Network. (2020, March 02). Retrieved from https://mc.ai/layers-of-a-convolutional-neural-network/

\item MACMAC 51644 silver badges1414 bronze badges. (1969, October 01). Understanding Gaussian Filter and how to plot it in 1-D. Retrieved from https://stackoverflow.com/questions/60462388/understanding-gaussian-filter-and-how-to-plot-it-in-1-d

\item Anh H. Reynolds. (2017, October 15). Convolutional Neural Networks (CNNs). Retrieved from https://anhreynolds.com/blogs/cnn.html

\item Diterbitkan oleh Benny Prijono Lihat semua pos dari Benny Prijono, Prijono, D., Prijono, B., Lihat semua pos dari Benny Prijono, 14, P., Berkata:, P., . . . (wajib), N. (2018, April 03). Student Notes: Convolutional Neural Networks (CNN) Introduction. Retrieved June 26, 2020, from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/

\item Sakshi TiwariCheck out this Author's contributed articles., Sakshi Tiwari, amp; Check out this Author's contributed articles. (2018, February 06). Activation functions in Neural Networks. Retrieved June 26, 2020, from https://www.geeksforgeeks.org/activation-functions-neural-networks/

\item Intro to Linear vs. Nonlinear Functions. (n.d.). Retrieved June 26, 2020, from https://www.expii.com/t/intro-to-linear-vs-nonlinear-functions-4318

\item V, A. (2017, March 30). Understanding Activation Functions in Neural Networks. Retrieved July 02, 2020, from https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0

\item KIDS, T. (2019, July 31). Fully-Connected Layer with dynamic input shape. Retrieved July 03, 2020, from https://medium.com/@tecokids.monastir/fully-connected-layer-with-dynamic-input-shape-70c869ae71af

\item Singh, S. (2019, March 02). Fully Connected Layer: The brute force layer of a Machine Learning model. Retrieved July 03, 2020, from https://iq.opengenus.org/fully-connected-layer/

\item Uniqtech. (2020, April 21). Understand the Softmax Function in Minutes. Retrieved July 03, 2020, from https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d

\item (PDF) A Deep Feature Learning Method for Drill Bits Monitoring Using the Spectral Analysis of the Acoustic Signals. (n.d.). Retrieved July 03, 2020, from https://www.researchgate.net/publication/327007086 A Deep Feature Learning Method for Drill Bits Monitoring Using the Spectral Analysis of the Acoustic Signals

\item Daniil's blog. (n.d.). Retrieved July 03, 2020, from http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/

\item 2018 Data Science Bowl. (n.d.). Retrieved July 06, 2020, from https://www.kaggle.com/c/data-science-bowl-2018/data

\item Jcoral. (2019, August 25). CamVid. Retrieved July 06, 2020, from https://www.kaggle.com/jcoral02/camvid

\item Badrinarayanan, Vijay, et al. “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.” ArXiv.org, 10 Oct. 2016, arxiv.org/abs/1511.00561.

\item Profile, bdd-data.berkeley.edu/portal.html.

\item Ronneberger, O., Fischer, P.; Brox, T. (2015, May 18). U-Net: Convolutional Networks for Biomedical Image Segmentation. Retrieved August 20, 2020, from https://arxiv.org/abs/1505.04597

\item Zhao, H., Qi, X., Shen, X., Shi, J.; Jia, J. (2018, August 20). ICNet for Real-Time Semantic Segmentation on High-Resolution Images. Retrieved August 20, 2020, from https://arxiv.org/abs/1704.08545

\item He, K., Zhang, X., Ren, S.,; Sun, J. (2015, December 10). Deep Residual Learning for Image Recognition. Retrieved August 20, 2020, from https://arxiv.org/abs/1512.03385
\end{enumerate}

\end{document}

